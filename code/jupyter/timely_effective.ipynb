{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---  \n",
    "title: \"Timely and effective care\"  \n",
    "subtitle: \"An analysis of the public dataset from CMS\"  \n",
    "date: last-modified  \n",
    "date-format: full  \n",
    "author:  \n",
    "  - name:  \n",
    "      given: Pranav Kumar  \n",
    "      family: Mishra  \n",
    "    affiliations:  \n",
    "      - ref: rushsurg  \n",
    "      - ref: rushortho  \n",
    "    corresponding: true  \n",
    "    url: https://drpranavmishra.com  \n",
    "    email: pranav_k_mishra@rush.edu  \n",
    "    orcid: 0000-0001-5219-6269  \n",
    "    role: \"Post Doctoral Research Fellow\"  \n",
    "format:  \n",
    "  html:  \n",
    "    code-fold: true  \n",
    "\n",
    "  pdf:\n",
    "    documentclass: scrartcl\n",
    "    toc: true\n",
    "    code-fold: true\n",
    "    papersize: letter\n",
    "    toc-depth: 2\n",
    "    margin-left: 30mm\n",
    "    margin-right: 30mm\n",
    "    highlight-style: github\n",
    "    colorlinks: true\n",
    "    default-image-extension: png \n",
    "execute:   \n",
    "  enabled: false  \n",
    "  echo: false  \n",
    "  output: asis #true  \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "import requests\n",
    "from io import StringIO\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import socket\n",
    "import pyreadstat\n",
    "\n",
    "\n",
    "import itertools\n",
    "import datalad.api as dl\n",
    "\n",
    "# Plotting\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.ticker import (\n",
    "    MultipleLocator,\n",
    "    FormatStrFormatter,\n",
    "    AutoMinorLocator,\n",
    "    MaxNLocator,\n",
    ")\n",
    "from matplotlib import ticker\n",
    "from matplotlib import image as image\n",
    "from matplotlib import cbook as cbook\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib.image import imread\n",
    "from matplotlib.dates import DateFormatter\n",
    "from matplotlib import rc, rcParams\n",
    "import cycler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image as ImagePIL\n",
    "from IPython.display import display\n",
    "from skimage.transform import resize\n",
    "from io import BytesIO\n",
    "\n",
    "from tabulate import tabulate\n",
    "import json\n",
    "\n",
    "# Date and time formatting\n",
    "import datetime as date\n",
    "\n",
    "# For grabbing URLs\n",
    "import urllib\n",
    "import math\n",
    "import re\n",
    "import csv\n",
    "\n",
    "# Map\n",
    "from geopy.geocoders import Nominatim\n",
    "import folium\n",
    "\n",
    "# Display\n",
    "from IPython.display import display, Markdown, Latex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General\n",
    "verbosity = True\n",
    "repository_root = \"/home/pranav/work/pranavmishra90/courses/by-mishra/research-data-management\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the computer name and the repository root\n",
    "\n",
    "# Get the name of the computer, then save it to an environment variable\n",
    "os.environ['computer_name']= socket.gethostname()\n",
    "\n",
    "if len(os.environ['computer_name']) == 12:\n",
    "    if os.path.exists('/home/coder/work'):\n",
    "        \n",
    "        is_docker = 'True'\n",
    "        os.environ['is_docker']=is_docker\n",
    "        os.chdir('/home/coder/work')\n",
    "        os.environ['repo_root']=os.getcwd()\n",
    "        print('This is a docker environment')\n",
    "\n",
    "    \n",
    "   \n",
    "elif os.path.exists(repository_root) == True:\n",
    "    is_docker = 'False'\n",
    "    os.environ['is_docker'] = is_docker\n",
    "    os.environ['repo_root'] = repository_root\n",
    "\n",
    "else:\n",
    "    is_docker = 'False'\n",
    "    os.environ['is_docker'] = is_docker\n",
    "    print(f\"Error finding the repository root for computer {os.environ['computer_name']}\")\n",
    "\n",
    "\n",
    "print(f\"The computer\\'s name is {os.environ['computer_name']}\")\n",
    "os.chdir(os.environ['repo_root'])\n",
    "print(f\"The current directory has been set to {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library of functions\n",
    "#!/usr/bin/env python\n",
    "#\n",
    "\n",
    "#\n",
    "# Created by:\n",
    "#\n",
    "# Pranav Kumar Mishra, MD\n",
    "# Post-Doctoral Research Fellow\n",
    "# Departments of Surgery and Orthopedic Surgery\n",
    "# Rush University Medical Center\n",
    "#\n",
    "# Email: pranav_k_mishra@rush.edu\n",
    "# Phone: 312-942-3146\n",
    "#\n",
    "# ### License: **MIT License**\n",
    "#\n",
    "# Copyright (c) 2023-2024 Pranav Kumar Mishra (Rush University, Chicago, IL)\n",
    "#\n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "# of this software and associated documentation files (the \"Software\"), to deal\n",
    "# in the Software without restriction, including without limitation the rights\n",
    "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "# copies of the Software, and to permit persons to whom the Software is\n",
    "# furnished to do so, subject to the following conditions:\n",
    "\n",
    "# The above copyright notice and this permission notice shall be included in all\n",
    "# copies or substantial portions of the Software.\n",
    "\n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "# SOFTWARE.\n",
    "\n",
    "# ## Import libraries\n",
    "\n",
    "# General\n",
    "import os\n",
    "import socket\n",
    "from IPython.display import display\n",
    "import datetime as date\n",
    "from copy import deepcopy\n",
    "import warnings\n",
    "import shutil\n",
    "from configparser import ConfigParser\n",
    "\n",
    "# Searching\n",
    "import json\n",
    "\n",
    "# Graphing\n",
    "import seaborn as sns\n",
    "from matplotlib.dates import DateFormatter\n",
    "from matplotlib.image import imread\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib import cbook as cbook\n",
    "from matplotlib import image as image\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import rc, rcParams\n",
    "\n",
    "# Data files\n",
    "import datalad.api as dl\n",
    "\n",
    "# Web\n",
    "import urllib\n",
    "import requests\n",
    "\n",
    "\n",
    "# Images\n",
    "from skimage.transform import resize\n",
    "from PIL import Image as ImagePIL\n",
    "\n",
    "# Statistics\n",
    "from scipy.stats import gmean, mannwhitneyu, normaltest\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "import csv\n",
    "import re\n",
    "import six\n",
    "\n",
    "# Progress bars\n",
    "stdout_fig_gen = False\n",
    "if stdout_fig_gen == False:\n",
    "    # Inline progress\n",
    "    from tqdm.auto import tqdm\n",
    "else:\n",
    "    # Popup progress\n",
    "    from tkinter import Tk\n",
    "    from tqdm.tk import tqdm\n",
    "\n",
    "# Date formatting\n",
    "execute_start_time = date.datetime.now()\n",
    "date_readable_format = \"%A, %b %d, %Y at %I:%M:%S %p\"\n",
    "date_machine_format = \"%Y-%m-%d_%H%M%S\"\n",
    "file_date = f\"{date.datetime.now().strftime('%Y-%m-%d')}\"\n",
    "\n",
    "\n",
    "# In[ ]: Create a python dictionary from a CSV file\n",
    "# -----------------------------------------------------------------------\n",
    "def csv_to_dict(csvfilepath):\n",
    "    # Open the CSV file and read its contents\n",
    "    with open(csvfilepath, \"r\") as file:\n",
    "        reader = csv.reader(file)\n",
    "\n",
    "        # Create a dictionary to hold the data\n",
    "        my_dict = {}\n",
    "\n",
    "        # Loop through each row of the CSV and add its data to the dictionary\n",
    "        for row in reader:\n",
    "            # print(row) #Uncomment this row for debugging\n",
    "            key = row[0]\n",
    "            value = row[1]\n",
    "            my_dict[key] = value\n",
    "\n",
    "        return my_dict\n",
    "\n",
    "\n",
    "# In[ ]: Fix nan inside of a dataframe\n",
    "# -----------------------------------------------------------------------\n",
    "def fix_nan(dataframe):\n",
    "    dataframe = dataframe.replace(r\"-99\", \"\", regex=True)\n",
    "    dataframe = dataframe.replace(-99, np.nan, regex=False)\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "# In[ ]: Within a dataframe's column, perform a find and replace\n",
    "# -----------------------------------------------------------------------\n",
    "def find_and_replace_df(input_df, column_name, input_sample, replace_sample):\n",
    "    df = deepcopy(input_df.loc[input_df[column_name] == input_sample])\n",
    "    df[column_name] = replace_sample\n",
    "\n",
    "    output_limited2 = input_df\n",
    "\n",
    "    output_limited2.loc[df.index] = np.nan\n",
    "\n",
    "    output_limited2 = output_limited2.combine_first(df)\n",
    "\n",
    "    return output_limited2\n",
    "\n",
    "\n",
    "# In[ ]: Change dataframe column dtypes according to a python dictionary\n",
    "# -----------------------------------------------------------------------\n",
    "def fix_column_dtype(dataframe, dtype_dict):\n",
    "    # List all of the columns in the dataframe\n",
    "    columns = dataframe.columns.tolist()\n",
    "\n",
    "    # Converting NaN values to -99\n",
    "    for col in dataframe.columns:\n",
    "        if pd.api.types.is_numeric_dtype(dataframe[col]):\n",
    "            # Fill NaN values with -99 for numeric columns\n",
    "            dataframe[col] = dataframe[col].fillna(-99)\n",
    "        elif dataframe[col].dtype == \"object\":\n",
    "            # Fill NaN values with '-99' for string columns\n",
    "            dataframe[col] = dataframe[col].fillna(\"-99\")\n",
    "\n",
    "    dataframe = dataframe.convert_dtypes(convert_floating=True)\n",
    "\n",
    "    for col in columns:\n",
    "        # Debugging -------\n",
    "        # If you are getting an error loading the dictionary file, uncommon the following print line. The last printed item is the problemmatic one in your dictionary of dtypes\n",
    "        # print(f\"Converting {col} to {dtype_dict[col]}\")\n",
    "        # -----------------\n",
    "        dataframe = dataframe\n",
    "        try:\n",
    "            dataframe[col] = dataframe[col].astype(dtype_dict[col])\n",
    "        except:\n",
    "            print(f\"Error changing dtype: {col} to {dtype_dict[col]}\")\n",
    "\n",
    "    nan_replaced = fix_nan(dataframe)\n",
    "\n",
    "    return nan_replaced  # Export the dataframe\n",
    "\n",
    "\n",
    "# In[ ]: Generate a matplotlib table 'figure' from a pandas dataframe\n",
    "# -----------------------------------------------------------------------\n",
    "def render_mpl_table(\n",
    "    data,\n",
    "    col_width=3.0,\n",
    "    row_height=0.625,\n",
    "    font_size=14,\n",
    "    header_color=\"#40466e\",\n",
    "    row_colors=[\"#f1f1f2\", \"w\"],\n",
    "    edge_color=\"w\",\n",
    "    bbox=[0, 0, 1, 1],\n",
    "    header_columns=0,\n",
    "    ax=None,\n",
    "    **kwargs,\n",
    "):\n",
    "    if ax is None:\n",
    "        size = (np.array(data.shape[::-1]) + np.array([0, 1])) * np.array(\n",
    "            [col_width, row_height]\n",
    "        )\n",
    "        fig, ax = plt.subplots(figsize=size)\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    mpl_table = ax.table(\n",
    "        cellText=data.values, bbox=bbox, colLabels=data.columns, **kwargs\n",
    "    )\n",
    "\n",
    "    mpl_table.auto_set_font_size(False)\n",
    "    mpl_table.set_fontsize(font_size)\n",
    "\n",
    "    for k, cell in six.iteritems(mpl_table._cells):\n",
    "        cell.set_edgecolor(edge_color)\n",
    "        if k[0] == 0 or k[1] < header_columns:\n",
    "            cell.set_text_props(weight=\"bold\", color=\"w\")\n",
    "            cell.set_facecolor(header_color)\n",
    "        else:\n",
    "            cell.set_facecolor(row_colors[k[0] % len(row_colors)])\n",
    "    return ax\n",
    "\n",
    "\n",
    "# In[ ]: Label bars with values\n",
    "# -----------------------------------------------------------------------\n",
    "def add_value_labels(ax1, label_size, spacing=5):\n",
    "    # \"\"\"Add labels to the end of each bar in a bar chart.\n",
    "\n",
    "    # Arguments:\n",
    "    #     ax (matplotlib.axes.Axes): The matplotlib object containing the axes\n",
    "    #         of the plot to annotate.\n",
    "    #     spacing (int): The distance between the labels and the bars.\n",
    "    # \"\"\"\n",
    "\n",
    "    # For each bar: Place a label\n",
    "    for rect in ax1.patches:\n",
    "        # Get X and Y placement of label from rect.\n",
    "        y_value = rect.get_height()\n",
    "        x_value = rect.get_x() + rect.get_width() / 2\n",
    "\n",
    "        # Number of points between bar and label. Change to your liking.\n",
    "        space = spacing\n",
    "        # Vertical alignment for positive values\n",
    "        va = \"bottom\"\n",
    "\n",
    "        # If value of bar is negative: Place label below bar\n",
    "        if y_value < 0:\n",
    "            # Invert space to place label below\n",
    "            space *= -1\n",
    "            # Vertically align label at top\n",
    "            va = \"top\"\n",
    "\n",
    "        # Use Y value as label and format number with one decimal place\n",
    "        label = \"{:.0f}\".format(y_value)\n",
    "\n",
    "        # Create annotation\n",
    "        ax1.annotate(\n",
    "            label,  # Use `label` as label\n",
    "            (x_value, y_value),  # Place label at end of the bar\n",
    "            xytext=(0, space),  # Vertically shift label by `space`\n",
    "            textcoords=\"offset points\",  # Interpret `xytext` as offset in points\n",
    "            ha=\"center\",  # Horizontally center label\n",
    "            va=va,\n",
    "            fontsize=label_size * 0.8,\n",
    "        )  # Vertically align label differently for\n",
    "        # positive and negative values.\n",
    "\n",
    "\n",
    "# In[ ]: Move files recursively\n",
    "# -----------------------------------------------------------------------\n",
    "def move_files_recursively(source, destination):\n",
    "    if not os.path.exists(destination):\n",
    "        os.makedirs(destination)\n",
    "\n",
    "    for item in os.listdir(source):\n",
    "        item_path = os.path.join(source, item)\n",
    "        destination_path = os.path.join(destination, item)\n",
    "\n",
    "        if os.path.isfile(item_path):\n",
    "            shutil.move(item_path, destination_path)\n",
    "        elif os.path.isdir(item_path):\n",
    "            shutil.move(item_path, destination_path)\n",
    "            move_files_recursively(destination_path, destination)\n",
    "\n",
    "\n",
    "def sort_json_dictionary(json_data):\n",
    "    data = json.loads(json_data)\n",
    "    sorted_data = {}\n",
    "    sorted_data = dict(sorted(data.items()))\n",
    "\n",
    "    for key, values in data.items():\n",
    "        sorted_data[key] = sorted(values)\n",
    "\n",
    "    sorted_json_data = json.dumps(sorted_data, indent=4)\n",
    "\n",
    "    return sorted_json_data\n",
    "\n",
    "\n",
    "def load_json_to_dict(file_path):\n",
    "    with open(file_path, \"r\") as file:\n",
    "        json_data = json.load(file)\n",
    "\n",
    "    sorted_data = {}\n",
    "    sorted_data = dict(sorted(json_data.items()))\n",
    "\n",
    "    for key, values in json_data.items():\n",
    "        sorted_data[key] = sorted(values)\n",
    "\n",
    "    sorted_json_data = json.dumps(sorted_data, indent=4)\n",
    "\n",
    "    return sorted_json_data\n",
    "\n",
    "\n",
    "# In[ ]: ######## Figure definitions and functions ##################\n",
    "# Formatting the following figures----------------------------------------\n",
    "\n",
    "title_fontsize = 20\n",
    "title_pad = 20\n",
    "\n",
    "axes_fontsize = 16\n",
    "axes_label_pad = 15\n",
    "label_size = 12\n",
    "xticks_size = 10\n",
    "\n",
    "fig_dpi = 75\n",
    "fig_display_dpi = 300\n",
    "fig_save_dpi = 300\n",
    "fig_save_dpi_scalar = 0.6\n",
    "fig_width = 10\n",
    "fig_height = 5\n",
    "data_source_alpha = 0.9\n",
    "\n",
    "image_filetype = \"png\"\n",
    "\n",
    "# Legend\n",
    "# plt.legend(loc=\"upper left\", bbox_to_anchor=(1, 1))\n",
    "\n",
    "# Watermarks\n",
    "project_name = \"Research Data Management\"\n",
    "created_by = \"Pranav Kumar Mishra\"\n",
    "subtitle_line1 = \"Post Doctoral Research Fellow\"\n",
    "subtitle_line2 = \"\"\n",
    "subtitle_line3 = \"Updated: \" + date.datetime.today().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "data_source = (\n",
    "    project_name\n",
    "    + \"\\n\"\n",
    "    + created_by\n",
    "    + \"\\n\"\n",
    "    + subtitle_line1\n",
    "    + \"\\n\"\n",
    "    + subtitle_line2\n",
    "    + \"\\n\"\n",
    "    + subtitle_line3\n",
    ")\n",
    "# data_source = \"\"\n",
    "\n",
    "# watermark_text = 'DRAFT'\n",
    "watermark_text = \"\"\n",
    "watermark_text_alpha = 0.2  # 0.2\n",
    "watermark_font_size = 100\n",
    "\n",
    "image_path = \"code/jupyter/rush-university-logo.png\"\n",
    "\n",
    "# Get the dimensions x, y of the image logo watermark\n",
    "pil_im = ImagePIL.open(image_path)\n",
    "pil_im = [int(i) for i in pil_im.size]\n",
    "\n",
    "dimensions = list(reversed(pil_im))\n",
    "dimensions = [int(dim) for dim in dimensions]\n",
    "image_size_scalar = (fig_save_dpi * fig_width / 5) / dimensions[1]\n",
    "dimensions_new = [int(dim * image_size_scalar) for dim in dimensions]\n",
    "\n",
    "# Load the image\n",
    "image_watermark = np.array(ImagePIL.open(image_path))\n",
    "\n",
    "image_watermark_alpha = 0.9  # 0.9\n",
    "\n",
    "# Resize the image\n",
    "image_watermark = resize(image_watermark, dimensions_new)\n",
    "# display(image_watermark.shape)\n",
    "\n",
    "rush_colors_hex = [\n",
    "    \"#00B480\",\n",
    "    \"#006332\",\n",
    "    \"#4EC5D8\",\n",
    "    \"#EA6852\",\n",
    "    \"#F2CD00\",\n",
    "    \"#006E96\",\n",
    "    \"#B12028\",\n",
    "    \"#F5A700\",\n",
    "]\n",
    "rush_cmap = ListedColormap(rush_colors_hex)\n",
    "\n",
    "file_date = f\"{date.datetime.now().strftime('%Y-%m-%d')}\"\n",
    "\n",
    "# Seaborn styling\n",
    "custom_params = {\n",
    "    \"axes.spines.right\": True,\n",
    "    \"axes.spines.top\": True,\n",
    "}\n",
    "sns.set_theme(style=\"white\", palette=\"muted\", rc=custom_params)\n",
    "\n",
    "file_date = f\"{date.datetime.now().strftime('%Y-%m-%d')}\"\n",
    "\n",
    "\n",
    "\n",
    "plt.close()\n",
    "\n",
    "# Legend\n",
    "legend_title_fontsize = 12\n",
    "\n",
    "\n",
    "# In [ ]: Label bars with values\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def add_value_labels(ax1, spacing=5):\n",
    "    \"\"\"Add labels to the end of each bar in a bar chart.\n",
    "\n",
    "    Arguments:\n",
    "        ax (matplotlib.axes.Axes): The matplotlib object containing the axes\n",
    "            of the plot to annotate.\n",
    "        spacing (int): The distance between the labels and the bars.\n",
    "    \"\"\"\n",
    "\n",
    "    # For each bar: Place a label\n",
    "    for rect in ax1.patches:\n",
    "        # Get X and Y placement of label from rect.\n",
    "        y_value = rect.get_height()\n",
    "        x_value = rect.get_x() + rect.get_width() / 2\n",
    "\n",
    "        # Number of points between bar and label. Change to your liking.\n",
    "        space = spacing\n",
    "        # Vertical alignment for positive values\n",
    "        va = \"bottom\"\n",
    "\n",
    "        # If value of bar is negative: Place label below bar\n",
    "        if y_value < 0:\n",
    "            # Invert space to place label below\n",
    "            space *= -1\n",
    "            # Vertically align label at top\n",
    "            va = \"top\"\n",
    "\n",
    "        # Use Y value as label and format number with one decimal place\n",
    "        label = \"{:.0f}\".format(y_value)\n",
    "\n",
    "        # Create annotation\n",
    "        ax1.annotate(\n",
    "            label,  # Use `label` as label\n",
    "            (x_value, y_value),  # Place label at end of the bar\n",
    "            xytext=(0, space),  # Vertically shift label by `space`\n",
    "            textcoords=\"offset points\",  # Interpret `xytext` as offset in points\n",
    "            ha=\"center\",  # Horizontally center label\n",
    "            va=va,\n",
    "            fontsize=label_size * 0.8,\n",
    "        )  # Vertically align label differently for\n",
    "        # positive and negative values.\n",
    "\n",
    "\n",
    "# In [ ]: Find a subdirectory given a pattern, using regex\n",
    "# ------------------------------------------------------------------------\n",
    "def find_subdirectory_with_pattern(input_path, pattern):\n",
    "    print(f\"Looking in directory: {input_path}\\nWith the pattern: {pattern}\\n\")\n",
    "    subdirectories = next(os.walk(input_path))[1]\n",
    "    regex_pattern = re.compile(pattern)\n",
    "\n",
    "    for subdirectory in subdirectories:\n",
    "        if regex_pattern.match(subdirectory):\n",
    "            # If the pattern matches, return out of the function\n",
    "            found_subdirectory = os.path.join(input_path, subdirectory)\n",
    "            print(f\"Found: {found_subdirectory}\")\n",
    "\n",
    "            return found_subdirectory\n",
    "\n",
    "    # Return nothing from the function outside of the 'for' loop\n",
    "    return None\n",
    "\n",
    "\n",
    "# In[ ]: Save a dataframe to the hard drive (CSV, parquet, etc.)\n",
    "# --------------------------------------------------------------\n",
    "def save_dataframe_table(df, table_filetypes_output):\n",
    "    for table_filetype in table_filetypes_output:\n",
    "        table_path = (\n",
    "            tables_base_path\n",
    "            + \"/\"\n",
    "            + output_directory_major\n",
    "            + output_filename_prefix\n",
    "            + output_filename_main\n",
    "            + output_filename_suffix\n",
    "            + f\".{table_filetype}\"\n",
    "        )\n",
    "\n",
    "        if table_filetype == \"csv\":\n",
    "            df.to_csv(table_path)\n",
    "\n",
    "        if table_filetype == \"parquet\":\n",
    "            df.to_parquet(table_path)\n",
    "    \n",
    "        if table_filetype == \"sav\":\n",
    "            pyreadstat.write_sav(df, table_path)\n",
    "\n",
    "\n",
    "# In[ ]: Calculate Mann-Whitney U Tests for all x-axis categories\n",
    "# ----------------------------------------------------------------\n",
    "def calc_stats_mwu(dataset, x_variable, y_variable):\n",
    "    visit_pairing = itertools.combinations(study_period_list, 2)\n",
    "\n",
    "    print(\"Mann-Whitney U Test\")\n",
    "    print(\"-------------------------------------------------------------------\")\n",
    "\n",
    "    print(f\"Testing significance between the variables:\")\n",
    "    print(f\" - `{x_variable}`\")\n",
    "    print(f\" - `{y_variable}`\")\n",
    "    print(\" \")\n",
    "    print(f\"selected_variable = `{selected_variable}`\")\n",
    "\n",
    "    print(\" \")\n",
    "    print(\" \")\n",
    "    print(\" \")\n",
    "\n",
    "    ## First print the statistically significant pairings\n",
    "    print(\"-------------------------------------------------------------------\")\n",
    "    print(f\"Statistically significant changes in {selected_variable} between:\")\n",
    "    print(\"-------------------------------------------------------------------\")\n",
    "\n",
    "    for combination in visit_pairing:\n",
    "        data1 = dataset[dataset[x_variable] == combination[0]][selected_variable]\n",
    "        data2 = dataset[dataset[x_variable] == combination[1]][selected_variable]\n",
    "\n",
    "        stat, p = stats.mannwhitneyu(data1, data2)\n",
    "\n",
    "        # interpret\n",
    "\n",
    "        if 0.01 <= p <= 0.05:\n",
    "            print(\n",
    "                f\"[*] Statistically SIGNIFICANT change in {selected_variable} between {combination[0]} and {combination[1]}. p = {round(p, 4)}\"\n",
    "            )\n",
    "            print(\" \")\n",
    "\n",
    "        elif 0.001 <= p < 0.01:\n",
    "            print(\n",
    "                f\"[**] Statistically SIGNIFICANT change in {selected_variable} between {combination[0]} and {combination[1]}. p = {round(p, 4)}\"\n",
    "            )\n",
    "            print(\" \")\n",
    "\n",
    "        elif p < 0.001:\n",
    "            print(\n",
    "                f\"[***] Statistically SIGNIFICANT change in {selected_variable} between {combination[0]} and {combination[1]}. p = {round(p, 4)}\"\n",
    "            )\n",
    "            print(\" \")\n",
    "\n",
    "    ## Then print the non-statistically significant pairings\n",
    "    print(\"-----------------------------------------------------------------------\")\n",
    "    print(f\"No statistically significant changes in {selected_variable} between:\")\n",
    "    print(\"-----------------------------------------------------------------------\")\n",
    "    visit_pairing = itertools.combinations(study_period_list, 2)\n",
    "    for combination in visit_pairing:\n",
    "        data1 = dataset[dataset[x_variable] == combination[0]][selected_variable]\n",
    "        data2 = dataset[dataset[x_variable] == combination[1]][selected_variable]\n",
    "\n",
    "        stat, p = stats.mannwhitneyu(data1, data2)\n",
    "        # interpret\n",
    "        if p > 0.05:\n",
    "            print(f\"{combination[0]} and {combination[1]} (p = {round(p, 4)})\")\n",
    "\n",
    "            print(\" \")\n",
    "\n",
    "\n",
    "# In[ ]: Drop columns by a regex name search\n",
    "# ----------------------------------------------------------------\n",
    "def drop_columns_regex (dataframe, regex):\n",
    "    \n",
    "    #List all of the columns in the dataframe\n",
    "    columns = dataframe.columns.tolist()\n",
    "\n",
    "    # display(regex)\n",
    "\n",
    "    deleted_columns = []\n",
    "\n",
    "    for col in columns:\n",
    "        if re.match(regex,col): #regex\n",
    "            deleted_columns.append([col])\n",
    "            dataframe = dataframe.drop([col], axis=1, inplace=False) # If regex matches, then remove the column\n",
    "\n",
    "    if verbosity == True:\n",
    "        display(f'The following columns have been deleted: {deleted_columns}')\n",
    "\n",
    "    return dataframe #Export the dataframe\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]: Comma formatted number in a string\n",
    "# ----------------------------------------------------------------\n",
    "\n",
    "def comma_formatted_num(number):\n",
    "\tformatted = ('{:,}'.format((number)))\n",
    "\n",
    "\treturn formatted\n",
    "\n",
    "def mddisplay(string):\n",
    "\tdisplay(Markdown(string))\n",
    "\n",
    "# In[ ]:  Function to calculate missing values by column\n",
    "# ----------------------------------------------------------------\n",
    "def missing_values_table(df):\n",
    "        # Total missing values\n",
    "        mis_val = df.isnull().sum()\n",
    "        \n",
    "        # Percentage of missing values\n",
    "        mis_val_percent = 100 * df.isnull().sum() / len(df)\n",
    "        \n",
    "        # Make a table with the results\n",
    "        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n",
    "        \n",
    "        # Rename the columns\n",
    "        mis_val_table_ren_columns = mis_val_table.rename(\n",
    "        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n",
    "        \n",
    "        # Sort the table by percentage of missing descending\n",
    "        mis_val_table_ren_columns = mis_val_table_ren_columns[\n",
    "            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n",
    "        '% of Total Values', ascending=False).round(1)\n",
    "        \n",
    "        # Print some summary information\n",
    "        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n",
    "            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n",
    "              \" columns that have missing values.\")\n",
    "        \n",
    "        # Return the dataframe with missing information\n",
    "        return mis_val_table_ren_columns\n",
    "######################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl.get(\"data/raw/cms/\")\n",
    "care_raw_df = pd.read_csv(\"data/raw/cms/Healthcare_Associated_Infections-Hospital.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "il_zip_df = pd.read_csv(\"code/library/zip-codes.csv\")\n",
    "il_zip_codes = il_zip_df['il_zip_codes'].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Run Parameters\n",
    "run_num = '001'\n",
    "\n",
    "zipcode_filter = il_zip_codes\n",
    "\n",
    "input_start_date = \"2022-07-01\"\n",
    "input_end_date = \"2024-12-01\"\n",
    "\n",
    "zip_start = 0\n",
    "zip_end = 99999\n",
    "\n",
    "\n",
    "# Figures\n",
    "image_filetypes_output = [\"jpg\"]\n",
    "data_source_alpha = 0\n",
    "watermark_text_alpha = 0\n",
    "watermarks = True\n",
    "errorbartype = (\"ci\", 95) \n",
    "\n",
    "save_figures = True\n",
    "\n",
    "# Tables\n",
    "table_filetypes_output = ['parquet', 'csv']\n",
    "save_tables = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_num = str(run_num).zfill(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_start_date = pd.to_datetime(input_start_date)\n",
    "filter_end_date = pd.to_datetime(input_end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories\n",
    "run_base_directory = \"data/analysis/timely_effective/runs\"\n",
    "\n",
    "this_run_directory = os.path.join(run_base_directory, \"run_\" + run_num)\n",
    "\n",
    "if not os.path.exists(this_run_directory):\n",
    "    print(f\"Creating a new directory at: {this_run_directory}\")\n",
    "    os.makedirs(this_run_directory)\n",
    "\n",
    "else:\n",
    "    # raise SystemExit(f\"Caution: The run directory already exists for Run # {run_num}\")\n",
    "\n",
    "    print(f\"Caution: The run directory already exists for Run # {run_num}\")\n",
    "\n",
    "# Figures directory\n",
    "figure_base_path = os.path.join(os.environ[\"repo_root\"], this_run_directory, \"figures/\")\n",
    "if not os.path.exists(figure_base_path):\n",
    "    os.makedirs(figure_base_path)\n",
    "else:\n",
    "    print(f\"A figures directory is already present: {figure_base_path}\")\n",
    "\n",
    "\n",
    "# Tables directory\n",
    "tables_base_path = os.path.join(os.environ[\"repo_root\"], this_run_directory, \"tables/\")\n",
    "if not os.path.exists(tables_base_path):\n",
    "    os.makedirs(tables_base_path)\n",
    "else:\n",
    "    print(f\"A tables directory is already present: {tables_base_path}\")\n",
    "\n",
    "# Notebook directory\n",
    "notebook_base_path = os.path.join(\n",
    "    os.environ[\"repo_root\"], this_run_directory, \"notebooks/\"\n",
    ")\n",
    "if not os.path.exists(notebook_base_path):\n",
    "    os.makedirs(notebook_base_path)\n",
    "else:\n",
    "    print(f\"A notebook directory is already present: {notebook_base_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Sanitization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "care_df_tmp = care_raw_df.copy()\n",
    "\n",
    "# Remove missing data\n",
    "care_df_tmp.loc[:, 'Score'] = care_df_tmp['Score'].replace(\"Not Available\", np.nan)\n",
    "care_df_tmp.loc[:, 'Score'] = care_df_tmp['Score'].replace(\"--\", np.nan)\n",
    "care_df_tmp['Score'] = pd.to_numeric(care_df_tmp['Score'], errors='coerce')\n",
    "\n",
    "# Set date formats to datetime\n",
    "care_df_tmp.loc[:, 'Start Date'] = pd.to_datetime(care_df_tmp['Start Date'], format='%m/%d/%Y')\n",
    "invalid_dates = care_df_tmp['Start Date'].isna()\n",
    "care_df_tmp = care_df_tmp[~invalid_dates]\n",
    "care_df_tmp.loc[:, 'End Date'] = pd.to_datetime(care_df_tmp['End Date'], format='%m/%d/%Y')\n",
    "invalid_dates = care_df_tmp['End Date'].isna()\n",
    "care_df_tmp = care_df_tmp[~invalid_dates]\n",
    "\n",
    "care_df_tmp[['Start Date', 'End Date']] = care_df_tmp[['Start Date', 'End Date']].astype('datetime64[ns]')\n",
    "\n",
    "# Drop extraneous columns\n",
    "care_df_tmp = care_df_tmp.drop(columns=['Address', 'Telephone Number'])\n",
    "\n",
    "care_df_tmp.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subset the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "care_df = care_df_tmp[(care_df_tmp[\"ZIP Code\"].isin(zipcode_filter)) & (care_df_tmp[\"Start Date\"] >= filter_start_date) & (care_df_tmp[\"End Date\"] <= filter_end_date)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(f\"\"\"# Filtering for Zipcode:\n",
    "\n",
    "The raw dataset `care_raw_df` has `{comma_formatted_num(care_raw_df['ZIP Code'].nunique())}` zip codes with `{comma_formatted_num(care_raw_df.shape[0])}` facilities.\n",
    "\n",
    "After filtering, `{comma_formatted_num(care_df['ZIP Code'].nunique())}` zip codes with `{comma_formatted_num(care_df.shape[0])}` facilities remain in the dataset `care_df`\n",
    "\"\"\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def var_exploration_continuous(df, exploration_variable_array):\n",
    "\n",
    "\tfor exploration_var in exploration_variable_array:\n",
    "\n",
    "\t\tmddisplay(f\"## Variable exploration: `{exploration_var}`\")\n",
    "\n",
    "\t\t# Descriptive statistics\n",
    "\t\tdisplay(pd.DataFrame(df[exploration_var].describe()))\n",
    "\n",
    "\t\tmdstring = f\"\"\"\n",
    "### Extremes of data\n",
    "\n",
    "Lowest Values:\n",
    "\n",
    "`{df[exploration_var].dropna().sort_values().head(10).to_list()}`\n",
    "\n",
    "Highest Values:\n",
    "\n",
    "`{df[exploration_var].dropna().sort_values().tail(10).to_list()}`\n",
    "\"\"\"\n",
    "\n",
    "\t\tdisplay(Markdown(mdstring))\n",
    "\n",
    "\t\tsns.displot(df, x=exploration_var, bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a variable to examine\n",
    "exploration_variables=['Score']\n",
    "\n",
    "var_exploration_continuous(care_df, exploration_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: IQR based outlier analysis\n",
    "def iqr_outlier_analysis(df, exploration_variables, display_outliers=True, apply_filter=False):\n",
    "\n",
    "    # Calculate first and third quartile\n",
    "    first_quartile = df[exploration_variables].quantile(0.25)\n",
    "    third_quartile = df[exploration_variables].quantile(0.75)\n",
    "\n",
    "    # Interquartile range\n",
    "    iqr = third_quartile - first_quartile\n",
    "\n",
    "\n",
    "    # Filter the data for outliers\n",
    "    df_outliers = df[\n",
    "        (df[exploration_variables] < (first_quartile - 3 * iqr))\n",
    "        | (df[exploration_variables] > (third_quartile + 3 * iqr))\n",
    "    ]\n",
    "\n",
    "    cases_before = df.shape[0]\n",
    "    cases_after = cases_before - df_outliers.shape[0]\n",
    "    cases_removed = cases_before - cases_after\n",
    "\n",
    "    # Print results\n",
    "    display(Markdown(f\"\"\"\n",
    "#### $3 \\\\times IQR$ Extremes for `{exploration_variables}`\n",
    "\n",
    "- Num cases before outlier filter: `{comma_formatted_num(cases_before)}`\n",
    "- IQR: `{round(iqr, 2)}` ({round(third_quartile, 2)} - {round(first_quartile, 2)})\n",
    "- Outliers: `{exploration_variables} < {round((first_quartile - (3 * iqr)), 2)}` OR `{exploration_variables} > {round((third_quartile + (3 * iqr)), 2)}`\n",
    "\"\"\"))\n",
    "    if display_outliers == True:\n",
    "        display(Markdown(\"**Outliers:**\"))\n",
    "        display(df_outliers)\n",
    "\n",
    "\n",
    "    if apply_filter == True:\n",
    "        # Apply filter\n",
    "        df = df[~df.index.isin(df_outliers.index)]\n",
    "        display(Markdown(f\"Cases after the outlier filter: {comma_formatted_num(cases_after)} (Removed {cases_removed})\"))\n",
    "    elif apply_filter==False:\n",
    "        display(Markdown(f\"Outlier filter not applied. We want to study the effects of `{exploration_variables}` on the model.\"))\n",
    "\n",
    "    # del data_outliers, cases_before, cases_after, cases_removed\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "care_df = iqr_outlier_analysis(care_df, \"Score\", display_outliers=False, apply_filter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Figures and Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earliest_date = care_df['Start Date'].min().strftime('%Y-%m-%d')\n",
    "latest_date = care_df['End Date'].max().strftime('%Y-%m-%d')\n",
    "\n",
    "\n",
    "run_details = f\"Run {run_num} Details:\\nDates: {earliest_date} - {latest_date}\\nNo. Zip Codes: {care_df['ZIP Code'].nunique()}\"\n",
    "\n",
    "print(run_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of Score\n",
    "\n",
    "# Define what will be plotted as the \"dataset\"\n",
    "# -------------------------------------------------\n",
    "selected_variable = \"Score\"\n",
    "title_prefix = r\"Distribution of \"\n",
    "\n",
    "selected_variable_title = \"  Score\"\n",
    "\n",
    "\n",
    "x_axis_title = r\"Score\"\n",
    "y_axis_title = r\"Number of Facilities\"\n",
    "\n",
    "\n",
    "output_directory_major = \"\"\n",
    "output_filename_prefix = f\"Run{run_num}_TE_\"\n",
    "output_filename_main = f\"Score_Dist\"\n",
    "\n",
    "output_filename_suffix = \"\"\n",
    "\n",
    "\n",
    "dataset = care_df.copy().dropna(subset=[selected_variable])[selected_variable].to_frame(selected_variable)\n",
    "\n",
    "\n",
    "# # --------------------------------------------------------------------------------------------\n",
    "\n",
    "fig1, _ = plt.subplots(figsize=(fig_width, fig_height), dpi=fig_display_dpi)\n",
    "\n",
    "plt.subplots_adjust(\n",
    "    left=None, bottom=None, right=None, top=None, wspace=None, hspace=None\n",
    ")\n",
    "\n",
    "# Histogram features\n",
    "# --------------------\n",
    "min_x_val = round(dataset.min().iloc[0], -1)\n",
    "max_x_val = round(dataset.max().iloc[0], -1)\n",
    "val_width = max_x_val - min_x_val\n",
    "bin_width = 5 # bin_width = val_width/n_bins\n",
    "n_bins = val_width / bin_width\n",
    "bin_range = (min_x_val, max_x_val)\n",
    "\n",
    "\n",
    "# Seaborn Histogram\n",
    "# --------------------\n",
    "ax1 = sns.histplot(\n",
    "    x=selected_variable,\n",
    "    data=dataset,\n",
    "    binwidth=bin_width,\n",
    "    binrange=bin_range,\n",
    "    color=rush_colors_hex[0],\n",
    ")\n",
    "\n",
    "\n",
    "# Plot traits\n",
    "# ------------------\n",
    "\n",
    "# Sample Size\n",
    "n_samples = str(dataset.shape[0])\n",
    "\n",
    "# Title\n",
    "fig1.suptitle(\n",
    "    selected_variable_title,\n",
    "    fontsize=title_fontsize,\n",
    "    fontweight=\"bold\",\n",
    "    y=1.05,\n",
    ")\n",
    "ax1.set_title(\n",
    "    \"(n=\" + str(n_samples) + \" facilities)     \",\n",
    "    size=title_fontsize / 1.75,\n",
    "    #   fontweight='bold',\n",
    "    pad=title_pad,\n",
    ")\n",
    "\n",
    "ax1.set_xlabel(x_axis_title, fontweight=\"bold\", size=axes_fontsize)\n",
    "ax1.set_ylabel(y_axis_title, fontweight=\"bold\", size=axes_fontsize)\n",
    "\n",
    "\n",
    "ax1.tick_params(labelsize=label_size)\n",
    "ax1.get_xaxis().tick_bottom()\n",
    "\n",
    "ax1.tick_params(which=\"minor\", length=0)\n",
    "ax1.yaxis.set_ticks_position(\"left\")\n",
    "ax1.yaxis.set_major_formatter(ticker.FormatStrFormatter(\"%d\"))\n",
    "\n",
    "\n",
    "ax1.xaxis.labelpad = axes_label_pad\n",
    "ax1.yaxis.labelpad = axes_label_pad\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.tick_params(which=\"major\", length=0)\n",
    "ax2.set_yticks([])\n",
    "ax2.set_ylabel(\" \", fontweight=\"bold\", size=axes_fontsize)\n",
    "ax2.yaxis.labelpad = axes_label_pad * 2\n",
    "\n",
    "# Watermarks\n",
    "if watermarks == True:\n",
    "    # Author\n",
    "    fig1.text(1,-0.1, data_source, color=\"black\", ha=\"right\", fontsize=6)\n",
    "    # Run Details\n",
    "    fig1.text(0.1, -0.1, run_details, color=\"black\", ha='left', va='bottom', fontsize=10, alpha=1)\n",
    "\n",
    "\n",
    "\n",
    "# Save the figure\n",
    "# ------------------\n",
    "if not os.path.exists(os.path.join(figure_base_path, output_directory_major)):\n",
    "    os.makedirs(os.path.join(figure_base_path, output_directory_major))\n",
    "\n",
    "\n",
    "for image_filetype in image_filetypes_output:\n",
    "    image_path = (\n",
    "        figure_base_path\n",
    "        + \"/\"\n",
    "        + output_directory_major\n",
    "        + output_filename_prefix\n",
    "        + output_filename_main\n",
    "        + output_filename_suffix\n",
    "        + f\".{image_filetype}\"\n",
    "    )\n",
    "\n",
    "    if save_figures == True:\n",
    "        plt.savefig(image_path, dpi=fig_save_dpi, bbox_inches=\"tight\")\n",
    "\n",
    "    else:\n",
    "        display(f\"The variable 'save_figures' is currently set to: {save_figures}\")\n",
    "\n",
    "# Clear the figure's dataset\n",
    "del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# City Count\n",
    "\n",
    "# Define what will be plotted as the \"dataset\"\n",
    "# -------------------------------------------------\n",
    "x_variable = \"City/Town\"\n",
    "title_prefix = r\"Distribution of \"\n",
    "\n",
    "selected_variable_title = \"City\"\n",
    "\n",
    "\n",
    "x_axis_title = r\"City\"\n",
    "y_axis_title = r\"Number of Facilities\"\n",
    "\n",
    "\n",
    "output_directory_major = \"\"\n",
    "output_filename_prefix = f\"Run{run_num}_TE_\"\n",
    "output_filename_main = f\"City Count\"\n",
    "\n",
    "output_filename_suffix = \"\"\n",
    "\n",
    "# --------------------------------------------------------------------------------------------\n",
    "# --------------------------------------------------------------------------------------------\n",
    "\n",
    "fig1, _ = plt.subplots(figsize=(fig_width, fig_height), dpi=fig_save_dpi)\n",
    "\n",
    "plt.subplots_adjust(\n",
    "    left=None, bottom=None, right=None, top=None, wspace=None, hspace=None\n",
    ")\n",
    "\n",
    "\n",
    "# Define what will be plotted as the \"dataset\"\n",
    "# -------------------------------------------------\n",
    "top_cities = care_df[x_variable].value_counts().head(5).index\n",
    "dataset = care_df[care_df[x_variable].isin(top_cities)].copy().dropna(subset=[selected_variable])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Seaborn\n",
    "# --------------------\n",
    "ax1 = sns.countplot(data=dataset, x=x_variable, orient='v', color=rush_colors_hex[0])\n",
    "\n",
    "\n",
    "\n",
    "# Plot traits\n",
    "# ------------------\n",
    "\n",
    "# Sample Size\n",
    "n_samples, var = dataset.shape\n",
    "n_samples = str(n_samples)\n",
    "\n",
    "\n",
    "# Title\n",
    "fig1.suptitle(\n",
    "    selected_variable_title, fontsize=title_fontsize, fontweight=\"bold\", y=1.05\n",
    ")\n",
    "ax1.set_title(\n",
    "    \"(n=\" + str(n_samples) + \" facilities)     \",\n",
    "    size=title_fontsize / 1.75,\n",
    "    #   fontweight='bold',\n",
    "    pad=title_pad,\n",
    ")\n",
    "\n",
    "\n",
    "ax1.set_xlabel(x_axis_title, fontweight=\"bold\", size=axes_fontsize)\n",
    "ax1.set_ylabel(y_axis_title, fontweight=\"bold\", size=axes_fontsize)\n",
    "\n",
    "ax1.tick_params(labelsize=label_size)\n",
    "ax1.get_xaxis().tick_bottom()\n",
    "\n",
    "ax1.tick_params(which=\"minor\", length=0)\n",
    "ax1.yaxis.set_ticks_position(\"left\")\n",
    "ax1.xaxis.labelpad = axes_label_pad\n",
    "ax1.yaxis.labelpad = axes_label_pad\n",
    "ax1.yaxis.set_major_formatter(FormatStrFormatter(\"%d\"))  # integer formatting\n",
    "\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.tick_params(which=\"major\", length=0)\n",
    "ax2.set_yticks([])\n",
    "ax2.set_ylabel(\" \", fontweight=\"bold\", size=axes_fontsize)\n",
    "ax2.yaxis.labelpad = axes_label_pad * 2\n",
    "\n",
    "\n",
    "# Watermarks\n",
    "if watermarks == True:\n",
    "    # Author\n",
    "    fig1.text(1,-0.1, data_source, color=\"black\", ha=\"right\", fontsize=6)\n",
    "    # Run Details\n",
    "    fig1.text(0.1, -0.1, run_details, color=\"black\", ha='left', va='bottom', fontsize=10, alpha=1)\n",
    "\n",
    "\n",
    "# Save the figure\n",
    "# ------------------\n",
    "\n",
    "if not os.path.exists(os.path.join(figure_base_path, output_directory_major)):\n",
    "    os.makedirs(os.path.join(figure_base_path, output_directory_major))\n",
    "\n",
    "for image_filetype in image_filetypes_output:\n",
    "    image_path = (\n",
    "        figure_base_path\n",
    "        + \"/\"\n",
    "        + output_directory_major\n",
    "        + output_filename_prefix\n",
    "        + output_filename_main\n",
    "        + output_filename_suffix\n",
    "        + f\".{image_filetype}\"\n",
    "    )\n",
    "\n",
    "    if save_figures == True:\n",
    "        plt.savefig(image_path, dpi=fig_save_dpi, bbox_inches=\"tight\")\n",
    "        display(f\"Figure saved to {image_path}\")\n",
    "    else:\n",
    "        display(f\"The variable 'save_figures' is currently set to: {save_figures}\")\n",
    "\n",
    "# Clear the figure's dataset\n",
    "#---------------------------\n",
    "del dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the run dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_tables = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframe to file\n",
    "output_directory_major = f\"/\"\n",
    "output_filename_prefix = f\"Run{run_num}_\"\n",
    "output_filename_main = f\"main_dataset\"\n",
    "output_filename_suffix = \"\"\n",
    "\n",
    "if save_tables == True:\n",
    "    save_dataframe_table(care_df, ['parquet'])\n",
    "else:\n",
    "        display(f\"The variable 'save_tables' is currently set to: {save_tables}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_df = care_df.sample(n=20)\n",
    "\n",
    "# Save demo_df as a csv\n",
    "output_directory_major = f\"/\"\n",
    "output_filename_prefix = f\"Run{run_num}_\"\n",
    "output_filename_main = f\"demo_selected\"\n",
    "output_filename_suffix = \"\"\n",
    "\n",
    "if save_tables == True:\n",
    "    save_dataframe_table(demo_df, ['csv'])\n",
    "else:\n",
    "        display(f\"The variable 'save_tables' is currently set to: {save_tables}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate run summary\n",
    "\n",
    "def list_files_as_markdown(directory):\n",
    "    markdown_list = \"\"\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            relative_path = os.path.relpath(os.path.join(root, file), directory)\n",
    "            markdown_list += f\"- {relative_path}\\n\"\n",
    "    return markdown_list\n",
    "\n",
    "def list_image_files_as_markdown(directory):\n",
    "    image_extensions = (\".png\", \".jpg\", \".svg\")\n",
    "    markdown_list = \"\"\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(image_extensions):\n",
    "                relative_path = os.path.relpath(os.path.join(root, file), directory)\n",
    "                markdown_list += f\"![{file}]({relative_path})\\n\\n\"\n",
    "    return markdown_list\n",
    "\n",
    "md_text = f\"\"\"---  \n",
    "title: Run {run_num} Summary\n",
    "subtitle: \"An analysis of the MBSA-QIP Dataset\"  \n",
    "date: last-modified  \n",
    "date-format: full  \n",
    "author:  \n",
    "  - name:  \n",
    "      given: Pranav Kumar  \n",
    "      family: Mishra  \n",
    "    affiliations:  \n",
    "      - ref: rushsurg  \n",
    "      - ref: rushortho  \n",
    "    corresponding: true  \n",
    "    url: https://drpranavmishra.com  \n",
    "    email: pranav_k_mishra@rush.edu  \n",
    "    orcid: 0000-0001-5219-6269    \n",
    "    role: \"Post Doctoral Research Fellow\"  \n",
    "format:  \n",
    "  html:  \n",
    "    code-fold: true  \n",
    "  pdf:  \n",
    "    documentclass: scrartcl  \n",
    "    toc-depth: 3  \n",
    "    code-fold: true  \n",
    "    highlight-style: github  \n",
    "    colorlinks: true  \n",
    "    tbl-cap-location: bottom  \n",
    "  gfm:  \n",
    "    preview-mode: raw  \n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "# Run {run_num} Summary\n",
    "\n",
    "Generated: {file_date}\n",
    "\n",
    "## Run Parameters:\n",
    "\n",
    "- Zip Codes: `{zip_start} - {zip_end}`\n",
    "\n",
    "\n",
    "## Dataset\n",
    "\n",
    "- Subjects: `{comma_formatted_num(care_df.shape[0])}`\n",
    "- [Main Dataset Parquet](data/analysis/timely_effective/runs/run_{run_num}/tables/Run{run_num}_main_dataset.parquet)\n",
    "- [Demo CSV - Random 20 Subjects](data/analysis/timely_effective/runs/run_{run_num}/tables/Run{run_num}_demo_selected.csv)\n",
    "\n",
    "## Figures\n",
    "\n",
    "{list_image_files_as_markdown(this_run_directory)}\n",
    "\n",
    "\n",
    "## Files\n",
    "\n",
    "The following files were generated from Run {run_num}:\n",
    "\n",
    "{list_files_as_markdown(this_run_directory)}\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# Save md_text to a run readme file\n",
    "\n",
    "quarto_path = os.path.join(os.environ['repo_root'], this_run_directory, \"readme.md\")\n",
    "\n",
    "with open(quarto_path, \"w\") as f:\n",
    "    f.write(md_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "researchdata",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
